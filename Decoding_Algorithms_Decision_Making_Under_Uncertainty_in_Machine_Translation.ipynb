{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNUrANpPk1aNXaS4x6XrFlH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Roxot/MTM-2025-lab/blob/main/Decoding_Algorithms_Decision_Making_Under_Uncertainty_in_Machine_Translation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decoding Algorithms: Decision Making Under Uncertainty in Machine Translation\n",
        "\n",
        "Machine translation systems are powered by neural networks trained to assign probabilities to possible target-language continuations, given a source-language input. Yet, while models learn to represent a distribution over candidate translations, our practical goal is typically to produce concrete translations. This gap is bridged by *decoding algorithms* — carefully designed procedures that explore the space of outcomes and make concrete decisions under uncertainty.\n",
        "\n",
        "Decoding algorithms come in different flavors:  \n",
        "- *Samplers* explore the space of outputs in line with the model’s probability distribution or restricted next-token distributions.  \n",
        "- *Search methods* attempt to identify the \"best\" sequence under some specified criterion.  \n",
        "\n",
        "In this lab, you will:  \n",
        "1. Implement a number of common samplers and decoding algorithms and examine their fundamental design.  \n",
        "2. Learn about decision rules that specify an \"optimal\" translation, focusing on MAP and MBR decoding.  \n",
        "3. Implement approximations to these two decision rules and assess their quality both as approximations to the target criterion and in terms of translation quality.  \n",
        "\n",
        "We will be using EuroLLM 1.7B Instruct, a multilingual instruction-tuned model developed as part of the UTTER partnership, to ground these exercises in a real translation system.  \n",
        "\n",
        "*This lab was developed in the context of the [UTTER Project](https://he-utter.eu/) (Grant Agreement No 101070631).*"
      ],
      "metadata": {
        "id": "VV2woXnyw48K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup\n"
      ],
      "metadata": {
        "id": "qOJVMkh3MdLB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers==4.37.2 datasets==2.16.0 huggingface-hub==0.23.0 accelerate==0.31.0\n",
        "!pip install sacremoses\n",
        "!pip install unbabel-comet"
      ],
      "metadata": {
        "id": "rFSlLO6mzAk2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from datasets import load_dataset\n",
        "\n",
        "import comet\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import logging, warnings\n",
        "import pytorch_lightning as pl\n",
        "from transformers import logging as hf_logging\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "logging.getLogger(\"pytorch_lightning\").setLevel(logging.ERROR)\n",
        "hf_logging.set_verbosity_error()\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "pl.seed_everything(42)"
      ],
      "metadata": {
        "id": "qJi4N_SQbr1r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The NMT Model\n",
        "\n",
        "We will use EuroLLM 1.7B Instruct as our working NMT system, a lightweight multilingual instruction-tuned model introduced in [EuroLLM (Martins et al., 2024)](https://arxiv.org/pdf/2409.16235). To run the following code efficiently, **make sure to select the correct Colab runtime** (GPU or TPU). The snippet below loads the model and tokenizer from Hugging Face Hub, moves the model to the selected device, and sets it to evaluation mode so we can focus on experimenting with samplers and decoding algorithms.\n",
        "\n",
        "The `ChatTemplate` helper class builds prompts in the format expected by EuroLLM’s chat interface. It wraps Hugging Face’s `apply_chat_template`, allowing us to provide either a single user string or a list of alternating user/assistant turns, with an optional system prompt. The output is a string formatted with this chat template configured for translation. We will set up the chat template with a system instruction to translate all texts to English, feel free to play around with this.\n",
        "\n",
        "In case no GPU is available, we will use a smaller more traditional translation system `Helsinki-NLP/opus-mt-fi-en` instead. In that case, the `ChatTemplate` class is a just dummy class to retain the same interface.\n"
      ],
      "metadata": {
        "id": "oDFGQzTRMmbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "    model_id = \"utter-project/EuroLLM-1.7B-Instruct\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16,\n",
        "        low_cpu_mem_usage=True\n",
        "    ).to(device).eval()\n",
        "\n",
        "    class ChatTemplate:\n",
        "        def __init__(self, tokenizer, system_prompt=None):\n",
        "            self.tokenizer = tokenizer\n",
        "            self.system_prompt = system_prompt\n",
        "\n",
        "        def __call__(self, x, add_generation_prompt=True, device=None):\n",
        "            def apply_chat_template(x_i):\n",
        "                messages = []\n",
        "                if self.system_prompt: messages.append({\"role\": \"system\", \"content\": self.system_prompt})\n",
        "                messages.append({\"role\": \"user\", \"content\": x_i})\n",
        "                return self.tokenizer.apply_chat_template(\n",
        "                    messages, add_generation_prompt=add_generation_prompt, tokenize=False\n",
        "                )\n",
        "\n",
        "            if isinstance(x, str):\n",
        "                return apply_chat_template(x)\n",
        "            elif isinstance(x, list):\n",
        "                conversations = []\n",
        "                for x_i in x:\n",
        "                    conversations.append(apply_chat_template(x_i))\n",
        "                return conversations\n",
        "            else:\n",
        "                raise ValueError(\"Expected string or list.\")\n",
        "else:\n",
        "    device = \"cpu\"\n",
        "    model_id = \"Helsinki-NLP/opus-mt-fi-en\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)\n",
        "    device = \"cpu\"\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "        model_id,\n",
        "        low_cpu_mem_usage=True\n",
        "    ).to(device).eval()\n",
        "\n",
        "    class ChatTemplate:\n",
        "        def __init__(self, tokenizer, system_prompt=None):\n",
        "            # dummy class for API compatability\n",
        "            pass\n",
        "\n",
        "        def __call__(self, x, add_generation_prompt=True):\n",
        "            return x\n",
        "\n",
        "chat_template = ChatTemplate(tokenizer, system_prompt=\"Translate all user texts to English.\")\n",
        "print(chat_template(\"Tervetuloa MT Marathon 2025 -tapahtumaan!\"))"
      ],
      "metadata": {
        "id": "NR8mrnxExCzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 1: Samplers\n",
        "\n",
        "We begin by implementing a set of common samplers that specify how the next token is chosen given the model’s probability distribution. Each of these methods is wrapped in a simple strategy class, which the `RolloutGenerator` can use to produce sequences token per token. We first implement them very verbosely for a single generation for didactic purposes. We start with the most pure form of sampling: unbiased / ancestral sampling.\n",
        "\n",
        "$$ y \\sim P_\\theta(Y|x) $$\n",
        "\n",
        "$Y$ represents the random variable over responses / generations, $\\theta$  are the trained neural network parameters, and $x$ is our source sentence. Note that for our instruction-tuned model, $x$ also includes the chat template and system prompt. From a probabilistic perspective, this does not change anything except for that we have a longer conditioning context.\n",
        "\n",
        "Generations produced with ancestral sampling follow the conditional distribution imposed by the neural network.In PyTorch we can use a nice helper function `torch.multinomial(.)` to perform the sampling operation for us."
      ],
      "metadata": {
        "id": "1aHRg7W7Ornm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutGenerator:\n",
        "    def __init__(self, model, tokenizer, strategy):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.strategy = strategy\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, input_ids, max_new_tokens=256):\n",
        "        if getattr(self.model.config, \"is_encoder_decoder\", False):\n",
        "            return self._generate_enc_dec(input_ids, max_new_tokens)\n",
        "        else:\n",
        "            return self._generate_dec_only(input_ids, max_new_tokens)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_dec_only(self, x, max_new_tokens):\n",
        "        \"\"\"\n",
        "        EuroLLM uses a decoder only architecture.\n",
        "        \"\"\"\n",
        "        context = self.tokenizer(x, return_tensors=\"pt\")[\"input_ids\"]\n",
        "        context = context.to(next(self.model.parameters()).device)\n",
        "        response = []\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.model(input_ids=context).logits[0, -1]  # [V]\n",
        "\n",
        "            # We call strategy for making a decision about the next token\n",
        "            next_token_id = self.strategy(logits) # int\n",
        "\n",
        "            response.append(next_token_id)\n",
        "            context = torch.cat([context,\n",
        "                                torch.tensor([[next_token_id]], device=context.device)], dim=1)\n",
        "\n",
        "            if next_token_id == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "        return torch.tensor(response, device=context.device)  # [response_length]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_enc_dec(self, x, max_new_tokens):\n",
        "        \"\"\"\n",
        "        If you're using a CPU and Helsinki-NLP/opus-mt-fi-en, this uses an encoder decoder architecture instead.\n",
        "        \"\"\"\n",
        "        input_ids = self.tokenizer(x, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "        decoder_input_ids = torch.tensor([[self.model.config.decoder_start_token_id]], device=device)\n",
        "        response = []\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.model(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits[0, -1]  # [V]\n",
        "\n",
        "             # We call strategy for making a decision about the next token\n",
        "            next_token_id = self.strategy(logits)\n",
        "            response.append(next_token_id)\n",
        "            decoder_input_ids = torch.cat([decoder_input_ids, torch.tensor([[next_token_id]], device=device)], dim=1)\n",
        "            if next_token_id == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "        return torch.tensor(response, device=device)  # [response_length]\n",
        "\n",
        "class AncestralSamplingStrategy:\n",
        "\n",
        "    def __call__(self, logits):  # logits: [V]\n",
        "        next_token_distribution = F.softmax(logits, dim=-1)\n",
        "        return torch.multinomial(next_token_distribution, 1)\n",
        "\n",
        "generator = RolloutGenerator(model, tokenizer, strategy=AncestralSamplingStrategy())\n",
        "x = chat_template(\"Tervetuloa MT Marathon 2025 -tapahtumaan!\")\n",
        "nsamples = 3\n",
        "for i in range(nsamples):\n",
        "    response_ids = generator.generate(x)\n",
        "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    print(f\"Sample {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "pA6mPrE-1pcL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look a bit more closely while rolling out these next token distributions and see what the next token distributions look like:"
      ],
      "metadata": {
        "id": "-zDfZGVkhy6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VerboseAncestralSamplingStrategy:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.prefix_response = []\n",
        "\n",
        "    def __call__(self, logits):\n",
        "        next_token_distribution = F.softmax(logits, dim=-1)\n",
        "        sampled_token_id = torch.multinomial(next_token_distribution, 1)\n",
        "        sampled_token = self.tokenizer.decode([sampled_token_id.item()])\n",
        "        print(f\"Partial response: {tokenizer.decode(self.prefix_response)}\")\n",
        "\n",
        "        top_probs, top_ids = torch.topk(next_token_distribution, k=5)\n",
        "\n",
        "        for token_prob, token_id in zip(top_probs, top_ids):\n",
        "            token_candidate = self.tokenizer.decode([token_id])\n",
        "            marker = \"←\" if token_id == sampled_token_id else \"\"\n",
        "            print(f\"'{token_candidate}'\\t\\t(p={token_prob:.4f}) {marker}\")\n",
        "        print(\"...\")\n",
        "\n",
        "        if sampled_token_id not in top_ids:\n",
        "            token_prob = float(next_token_distribution[sampled_token_id.item()])\n",
        "            print(f\"{sampled_token}\\t\\t(p={token_prob:.4f}) ←\")\n",
        "        print()\n",
        "\n",
        "        self.prefix_response.append(sampled_token_id.item())\n",
        "\n",
        "        return sampled_token_id\n",
        "\n",
        "x = chat_template(\"Tervetuloa MT Marathon 2025 -tapahtumaan!\")\n",
        "generator = RolloutGenerator(model, tokenizer, strategy=VerboseAncestralSamplingStrategy(tokenizer))\n",
        "response_ids = generator.generate(x)\n",
        "response = tokenizer.decode(response_ids, skip_special_tokens=True)"
      ],
      "metadata": {
        "id": "DqwtP_grdEZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Top-k Sampling\n",
        "As you may have noticed, unbiased / ancestral samples are not always consistently of high quality, as low probability responses may result in poor translations. In an attempt to improve this a number of truncated samplers have been proposed that restrict the support of the next token distributions. Top-k samplings ([Fan et al., 2018](https://aclanthology.org/P18-1082/)) restricts the vocabulary of next token distributions to the $k$ most probable tokens. Borrowing the formulation of [Meister et al.](https://aclanthology.org/2023.tacl-1.7.pdf), we can define the *truncation set* $C_k(t)$ as the solution to the following optimization problem:\n",
        "\n",
        "\\begin{aligned}\n",
        "    \\max_{C_k(t) \\in \\mathcal{P}(V)} \\quad\n",
        "    & \\sum_{y \\in C_k(t)} P_\\theta(y \\mid x, y_{<t}) \\\\\n",
        "    \\text{subject to} \\quad\n",
        "    & |C_k(t)| \\leq k,\n",
        "\\end{aligned}\n",
        "\n",
        "\n",
        "where $\\mathcal{P}$ is the power set operator and $V$ is our vocabulary. The solution to this optimization problem is trivially the $k$ highest probability tokens of the next token distribution. In top-k sampling, we then sample from the conditional distribution:\n",
        "\n",
        "$$ Y_t^\\text{top-k} \\sim P_\\theta(Y_t=y_t \\mid x, y_{<t}, y_t \\in C_k(t)). $$\n",
        "\n",
        "Implement this strategy below."
      ],
      "metadata": {
        "id": "oyXYmZYPwFbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TopKSamplingStrategy:\n",
        "\n",
        "    def __init__(self, k=50):\n",
        "        self.k = k\n",
        "\n",
        "    def __call__(self, logits): # logits: [V]\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "        Hint: you can use torch.topk(.) to easily get the top-k IDs and their corresponding probabilities.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "k = 50\n",
        "generator = RolloutGenerator(model, tokenizer, strategy=TopKSamplingStrategy(k=k))\n",
        "nsamples = 3\n",
        "for i in range(nsamples):\n",
        "    response_ids = generator.generate(x)\n",
        "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    print(f\"Top-k (k={k}) sample {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "Hg8BW5FSWMHP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Nucleus Sampling\n",
        "Now we move on to a sampler that adapts the candidate set size flexibly following the shape of the next-token distribution. Nucleus sampling ([Holtzman et al., 2020](https://openreview.net/forum?id=rygGQyrFvH)) defines the truncation set as the solution to the following optimization problem:\n",
        "\n",
        "\\begin{aligned}\n",
        "    \\min_{\\mathcal{C}_p(t) \\in \\mathcal{P}(V)} \\quad\n",
        "    & \\lvert \\mathcal{C}_t \\rvert \\\\\n",
        "    \\text{subject to} \\quad\n",
        "    & \\sum_{y \\in \\mathcal{C}_p(t)} P_\\theta(y \\mid x, y_{<t}) \\;\\geq\\; p\n",
        "\\end{aligned}\n",
        "\n",
        "where we have hyperparameter $p \\in (0, 1)$. The solution to this optimization problem is the smallest set of tokens (sorted by probability) whose cumulative mass reaches at least $p$. Then, nucleus sampling samples from the distribution\n",
        "\n",
        "$$Y_t^{\\text{nucleus}} \\sim P_\\theta(Y_t=y_t \\mid x, y_{<t}, \\ y_t \\in C_p(t)).$$\n",
        "\n",
        "Implement this strategy below.\n"
      ],
      "metadata": {
        "id": "VjTpRF3vy-ys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class NucleusSamplingStrategy:\n",
        "\n",
        "    def __init__(self, p=0.9):\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, logits): # logits: [V]\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "        Hint: You can use torch.cumsum(.) to compute cumulative probability mass.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "p = 0.9\n",
        "generator = RolloutGenerator(model, tokenizer, strategy=NucleusSamplingStrategy(p=p))\n",
        "nsamples = 3\n",
        "for i in range(nsamples):\n",
        "    response_ids = generator.generate(x)\n",
        "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    print(f\"Nucleus (p={p}) sample {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "LXJI8034y9BZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Locally Typical Sampling\n",
        "Finally, we have a look at a perhaps slightly lesser known sampling strategy: locally typical sampling ([Meister et al., 2023](https://aclanthology.org/2023.tacl-1.7.pdf)). Locally typical sampling defines the truncation set as the solution to the optimization problem:\n",
        "\n",
        "\\begin{aligned}\n",
        "    \\min_{\\mathcal{C}_\\tau(t) \\in \\mathcal{P}(V)} \\quad\n",
        "    & \\sum_{y \\in \\mathcal{C}_\\tau(t)} \\Bigl\\lvert \\, H(Y_t \\mid x, y_{<t}, \\theta))\n",
        "      - \\log \\left(\\frac{1}{P_\\theta(Y_t = y \\mid x, y_{<t})}\\right) \\, \\Bigr\\rvert \\\\\n",
        "    \\text{subject to} \\quad\n",
        "    & \\sum_{y \\in \\mathcal{C}_\\tau(t)} P_\\theta(Y_t = y \\mid x, y_{<t}) \\geq \\tau,\n",
        "\\end{aligned}\n",
        "\n",
        "where $H(Y_t\\mid x, y_{<t}, \\theta)$ is the entropy of the next token distribution.\n",
        "\n",
        "In practice, the resulting set is the smallest set of tokens sorted by the absolute distance of their *surprisal values* to the entropy, whose cumulative probability mass is at least $\\tau$. Surprisal is a concept from information theory that measures how \"unexpected\" a token $v$ is under the model distribution:\n",
        "\n",
        "\\begin{aligned}\n",
        "s(y_t \\mid x, y_{<t}, \\theta) &= \\log\\left(\\frac{1}{P_\\theta(Y_t=v|x, y_{<t})}\\right) \\\\\n",
        "&=  -\\log P_\\theta(Y_t=v|x, y_{<t})\n",
        "\\end{aligned}\n",
        "\n",
        "The entropy of the predictive distribution is the expected surprisal:\n",
        "\n",
        "$$\n",
        "H(Y_t \\mid x, y_{<t}, \\theta) = \\mathbb{E}_{P_\\theta(Y_t \\mid x, y_{<t})}[s(y_t \\mid x, y_{<t}, \\theta)] \\, .\n",
        "$$\n",
        "\n",
        "Locally typical sampling then selects tokens whose surprisal $s(y \\mid y_{<t})$ is within $\\tau$ distance to the entropy of the next token distribution.\n",
        "\n",
        "\n",
        "$$Y_t^{\\text{typical}} \\sim P_\\theta(Y_t=y_t \\mid x, y_{<t},\\ y_t \\in \\mathcal{C}_\\tau(t)).$$\n",
        "\n",
        "Note that ${C}_\\tau(t)$ does not necessarily include the highest probability tokens.\n",
        "\n",
        "Implement this strategy below.\n"
      ],
      "metadata": {
        "id": "RRCpYE5Hy8yx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LocallyTypicalSamplingStrategy:\n",
        "\n",
        "    def __init__(self, tau=0.2):\n",
        "        self.tau = tau\n",
        "\n",
        "    def __call__(self, logits):\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "        Hint: Be careful to avoid NaNs when using torch.log(.), add some small constant (e.g. 1e-12) to avoid log(0) or use logsoftmax.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "tau = 0.95\n",
        "generator = RolloutGenerator(model, tokenizer, strategy=LocallyTypicalSamplingStrategy(tau=tau))\n",
        "nsamples = 3\n",
        "for i in range(nsamples):\n",
        "    response_ids = generator.generate(x)\n",
        "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    print(f\"Locally Typical (tau={tau}) sample {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "HjfcGB1RRoqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Batched implementation\n",
        "\n",
        "So far, you have implemented ancestral sampling, top-$k$, nucleus, and locally typical sampling on a simple one-dimensional example. This is nice for didactic purposes, but usually batched implementations are used for efficiency. This requires your sampling functions to handle inputs of shape `[B, V]` instead of `[V]`, returning one token per batch element.\n",
        "\n",
        "\n",
        "Below, adapt your ancestral sampling implementation to work for batches as well in `AncestralSamplingStrategyBatched`. Optionally, you may also do this for the other sampling strategies."
      ],
      "metadata": {
        "id": "DLH4d63-1s74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RolloutGeneratorBatched:\n",
        "    def __init__(self, model, tokenizer, strategy):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.strategy = strategy\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, x, max_new_tokens=256):\n",
        "        if getattr(self.model.config, \"is_encoder_decoder\", False):\n",
        "            return self._generate_enc_dec(x, max_new_tokens)\n",
        "        else:\n",
        "            return self._generate_dec_only(x, max_new_tokens)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_dec_only(self, x, max_new_tokens):\n",
        "        device = next(self.model.parameters()).device\n",
        "        toks = self.tokenizer(x, return_tensors=\"pt\", padding=True)\n",
        "        context = toks[\"input_ids\"].to(device) # [B, S]\n",
        "        attn = toks[\"attention_mask\"].to(device) # [B, S]\n",
        "        response = torch.zeros((context.size(0), 0), dtype=torch.long, device=device)\n",
        "        B = context.size(0)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.model(input_ids=context, attention_mask=attn).logits[:, -1] # [B, V]\n",
        "            next_id = self.strategy(logits) # [B]\n",
        "            next_id = torch.where(finished, torch.full_like(next_id, self.tokenizer.eos_token_id), next_id)\n",
        "\n",
        "            context = torch.cat([context, next_id.unsqueeze(-1)], dim=1)\n",
        "            attn = torch.cat([attn, torch.ones(B, 1, dtype=attn.dtype, device=device)], dim=1)\n",
        "            response = torch.cat([response, next_id.unsqueeze(-1)], dim=1)\n",
        "\n",
        "            finished |= (next_id == self.tokenizer.eos_token_id)\n",
        "            if finished.all(): break\n",
        "\n",
        "        return response  # [B, T]\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def _generate_enc_dec(self, x, max_new_tokens):\n",
        "        device = next(self.model.parameters()).device\n",
        "        toks = self.tokenizer(x, return_tensors=\"pt\", padding=True)\n",
        "        enc = toks[\"input_ids\"].to(device) # [B, S]\n",
        "        enc_attn = toks[\"attention_mask\"].to(device) # [B, S]\n",
        "        B = enc.size(0)\n",
        "        start_id = self.model.config.decoder_start_token_id\n",
        "        dec = torch.full((B, 1), start_id, dtype=torch.long, device=device)\n",
        "        response = torch.zeros((B, 0), dtype=torch.long, device=device)\n",
        "        finished = torch.zeros(B, dtype=torch.bool, device=device)\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            logits = self.model(input_ids=enc, attention_mask=enc_attn,\n",
        "                                decoder_input_ids=dec).logits[:, -1] # [B, V]\n",
        "            next_id = self.strategy(logits) # [B]\n",
        "            next_id = torch.where(finished, torch.full_like(next_id, self.tokenizer.eos_token_id), next_id)\n",
        "\n",
        "            dec = torch.cat([dec, next_id.unsqueeze(-1)], dim=1)\n",
        "            response = torch.cat([response, next_id.unsqueeze(-1)], dim=1)\n",
        "\n",
        "            finished |= (next_id == self.tokenizer.eos_token_id)\n",
        "            if finished.all(): break\n",
        "\n",
        "        return response  # [B, T]\n",
        "\n",
        "class AncestralSamplingStrategyBatched:\n",
        "\n",
        "    def __call__(self, logits): # logits: [B, V]\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "        Hint: you can implement this almost identically to before.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class TopKSamplingBatched:\n",
        "\n",
        "    def __call__(self, logits): # logits: [B, V]\n",
        "        \"\"\"\n",
        "        (Optional) Implement your solution here.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class NucleusSamplingBatched: # logits: [B, V]\n",
        "\n",
        "    def __call__(self, logits):\n",
        "        \"\"\"\n",
        "        (Optional) Implement your solution here.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class LocallyTypicalSamplingBatched:\n",
        "\n",
        "    def __call__(self, logits): # logits: [B, V]\n",
        "        \"\"\"\n",
        "        (Optional) Implement your solution here.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "generator = RolloutGeneratorBatched(model, tokenizer, strategy=AncestralSamplingStrategyBatched())\n",
        "x = chat_template([\"Tervetuloa MT Marathon 2025 -tapahtumaan!\"] * 3)\n",
        "response_ids = generator.generate(x)\n",
        "responses = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Sample {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "kwYdcW7YiGEy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 2: Maximum-A-Posteriori (MAP)\n",
        "\n",
        "So far, we have examined various sampling-based decoding strategies. While these can yield fluent and diverse generations, they do not explicitly approximate any particular decision rule. In contrast, **maximum-a-posteriori (MAP)** decoding corresponds to a well-defined decision rule: selecting the single most probable sequence under the model distribution. MAP serves as the implicit principle behind many widely used decoding strategies:\n",
        "\n",
        "$$\n",
        "y^{\\text{MAP}} = \\arg\\max_{y \\in \\mathcal{Y}} P_\\theta(Y=y \\mid x)\n",
        "$$\n",
        "\n",
        "Solving for this optimization problem exactly is intractable, given the unbounded nature of the outcome space. Therefore, approximate decoding algorithms are used to approximate the decision rule.\n",
        "\n",
        "Greedy decoding is one such simple approximation. Greedy decoding selects the locally highest-probability token at each step, always choosing the most likely continuation given the current context.\n",
        "\n",
        "Implement this strategy below for both the 1D case and the batched case. Additionally, for the non-batched case, also compute the log probability of the entire sequence, i.e. the log probability that the model assigns to the outcome. As to avoid having to override the RolloutGenerator function, we'll do this a bit hacky by keeping a state in the generation strategy and reading this out after generating. Remember, we factorize this probability as:\n",
        "\n",
        "$$ P(y|x, \\theta) = \\prod_{t=1, 2\\ldots|y|} P_\\theta(y_t|x) $$"
      ],
      "metadata": {
        "id": "Lh46uLSdJQ-S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GreedyStrategy:\n",
        "\n",
        "    log_probs = None\n",
        "\n",
        "    def get_log_probs(self):\n",
        "        log_probs = self.log_probs.cpu().numpy()\n",
        "        self.log_probs = None\n",
        "        return log_probs\n",
        "\n",
        "    def __call__(self, logits): # logits: [V]\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "\n",
        "        Return the chosen token ID according to the greedy decoding strategy,\n",
        "        and update the log probability of the sequences in self.log_probs.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "class GreedyStrategyBatched:\n",
        "\n",
        "    def __call__(self, logits): # logits: [B, V]\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "\n",
        "        Return the chosen token ID according to the greedy decoding strategy.\n",
        "        Don't worry about computing log probs here as you are missing some state\n",
        "        variables to keep track of finished sequences when sequences in the batch\n",
        "        are different lengths.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "# Non-batched, with log probs\n",
        "x = chat_template([\"Tervetuloa MT Marathon 2025 -tapahtumaan!\",\n",
        "                   \"Tervetuloa MT Marathon 2025:een, viikon mittaiseen konekäännösteknologian huipputapahtumaan, joka järjestetään 25.–29. elokuuta Helsingin yliopiston kampuksella ydinkeskustassa!\"])\n",
        "greedy_strategy = GreedyStrategy()\n",
        "generator = RolloutGenerator(model, tokenizer, strategy=greedy_strategy)\n",
        "for x_i in x:\n",
        "    response_ids = generator.generate(x_i)\n",
        "    log_probs = greedy_strategy.get_log_probs()\n",
        "    response = tokenizer.decode(response_ids, skip_special_tokens=True)\n",
        "    print(response)\n",
        "    print(log_probs)\n",
        "\n",
        "# Batched\n",
        "generator = RolloutGeneratorBatched(model, tokenizer, strategy=GreedyStrategyBatched())\n",
        "response_ids = generator.generate(x)\n",
        "responses = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
        "print(responses)"
      ],
      "metadata": {
        "id": "rfnxH304I2ob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Beam Search\n",
        "Beam search approximates MAP decoding by expanding greedy search from 1 path to k paths. At each step, it expands each partial hypothesis with all candidate tokens, scores them by cumulative log-probability (optionally with some length normalization/penalty, which we will not use), and prunes to the top-k continuations. This breadth-limited search can potentially find higher probability responses than greedy search by keeping multiple promising alternatives in play.\n",
        "\n",
        "Implementing a correct, batched beam search is fiddly and quite outside of the scope of this lab. Therefore, we just use the `transformers` library its built-in implementation. If you want to study a clean reference implementation, I recommend having a look at [Joey NMT](https://github.com/joeynmt/joeynmt/tree/main). We also return the log probability again, but this time we conveniently get this as an output of the beam search call.\n",
        "\n",
        "Can you find a higher probability outcome than with greedy decoding by increasing the beam size?"
      ],
      "metadata": {
        "id": "kY4TqIbkpdNW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def beam_search_generate(x, model, tokenizer, beam_size, max_new_tokens=256, return_log_probs=False):\n",
        "    torch.cuda.empty_cache()\n",
        "    device = next(model.parameters()).device\n",
        "    tokens = tokenizer(x, return_tensors=\"pt\", padding=True).to(device)\n",
        "    padded_input_len = tokens[\"input_ids\"].shape[1]\n",
        "    out = model.generate(\n",
        "        **tokens,\n",
        "        num_beams=beam_size,\n",
        "        do_sample=False,\n",
        "        early_stopping=True,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        output_scores=return_log_probs,\n",
        "        length_penalty=0.0, # disable length penalty enabled by default\n",
        "        return_dict_in_generate=return_log_probs\n",
        "    )\n",
        "\n",
        "    if return_log_probs:\n",
        "        response_ids = out[\"sequences\"].cpu().numpy()\n",
        "        log_probs = out[\"sequences_scores\"].cpu().numpy()\n",
        "\n",
        "        del out.scores\n",
        "        del out\n",
        "    else:\n",
        "        response_ids = out.cpu().numpy()\n",
        "\n",
        "    # Remove the prompt for decoder-only models\n",
        "    if not getattr(model.config, \"is_encoder_decoder\", False):\n",
        "        response_ids = [rids[padded_input_len:] for rids in response_ids]\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if return_log_probs:\n",
        "        return response_ids, log_probs\n",
        "\n",
        "    return response_ids\n",
        "\n",
        "response_ids, log_probs = beam_search_generate(x, model, tokenizer, beam_size=2, return_log_probs=True)\n",
        "responses = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
        "print(responses)\n",
        "print(log_probs)"
      ],
      "metadata": {
        "id": "QLIBZja6I2l8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FLORES-200 (fi→en)\n",
        "\n",
        "We now switch from toy examples to real translation data. The code below loads a tiny Finnish→English subset of FLORES-200 (default: `devtest`, first $n$ examples). We also set up some lightweight evaluation using Cometinho."
      ],
      "metadata": {
        "id": "gOwBvNNZtOzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_flores_fi_en_subset(n=25, split=\"devtest\"):\n",
        "    \"\"\"\n",
        "    Load a tiny fi→en subset from FLORES-200.\n",
        "    \"\"\"\n",
        "    cfg = \"fin_Latn-eng_Latn\"\n",
        "    dataset = load_dataset(\"facebook/flores\", name=cfg, split=split, trust_remote_code=True)\n",
        "    if n is not None and n > 0:\n",
        "        dataset = dataset.select(range(min(n, len(dataset))))\n",
        "\n",
        "    src = dataset[\"sentence_fin_Latn\"]\n",
        "    ref = dataset[\"sentence_eng_Latn\"]\n",
        "\n",
        "    return src, ref\n",
        "\n",
        "comet_scorer = comet.load_from_checkpoint(comet.download_model(\"Unbabel/eamt22-cometinho-da\"))\n",
        "def score_translations(src, refs, preds):\n",
        "    data = [{\"src\": s, \"mt\": p, \"ref\": r} for s, r, p in zip(src, refs, preds)]\n",
        "    out = comet_scorer.predict(\n",
        "        data,\n",
        "        batch_size=32,\n",
        "        gpus=0\n",
        "    )\n",
        "    return out[\"system_score\"] * 100.0\n",
        "\n",
        "source, references = load_flores_fi_en_subset()\n",
        "x = chat_template(source[:3])\n",
        "response_ids = beam_search_generate(x, model, tokenizer, beam_size=5)\n",
        "predictions = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
        "\n",
        "for src, ref, pred in zip(source[:3], references[:3], predictions):\n",
        "    print(f\"Source: {src}\")\n",
        "    print(f\"Reference: {ref}\")\n",
        "    print(f\"Prediction: {pred}\")\n",
        "    print()\n",
        "\n",
        "score = score_translations(source[:3], references[:3], predictions)\n",
        "print(f\"\\nCometinho score: {score:.2f}\")"
      ],
      "metadata": {
        "id": "iDXv7qmwI2jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Larger Beam Sizes for Better Approximating MAP\n",
        "\n",
        "Beam search with a larger beam tends to find translations with higher probability, that thus approximate the MAP objective better. In this exercise experiment with increasing the beam size and study how this affects its approximation to the MAP objective, as well as how this influences translation quality.\n",
        "\n",
        "In the code cell below, do the following:\n",
        "- Generate translations with several increasing beam size.\n",
        "- Compute the *total log-probability* of each translation under the model (conditioned on the prompt).\n",
        "- Compute the Cometinho score for the predictions, and observe how these change as the beam widens.\n",
        "\n",
        "Implement your solution below."
      ],
      "metadata": {
        "id": "3zY_C7B3uTYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_total_logprob = []  # mean over dataset of total (summed) log-prob\n",
        "cometinho = []              # system cometinho scores\n",
        "beam_sizes = [2, 5, 8]\n",
        "\n",
        "\"\"\"\n",
        "Implement your solution here.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "HpJDC36gI2Wp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the system Cometinho score and average log probability as a function of beam size. If all went well, we should expect to see our approximation to the MAP objective get better for larger beam sizes. Cometinho scores, however, do not necessarily need to follow these. In fact, for larger beam sizes they often do not (\"The Beam Search Curse\", [Koehn and Knowles, 2017](https://aclanthology.org/W17-3204.pdf))."
      ],
      "metadata": {
        "id": "orUpCodEnDnp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.plot(beam_sizes, average_total_logprob, marker=\"o\")\n",
        "ax.set_xlabel(\"Beam size\")\n",
        "ax.set_ylabel(\"Average total log-prob\")\n",
        "ax.set_title(\"Beam size vs. log-prob\")\n",
        "\n",
        "ax = axes[1]\n",
        "ax.plot(beam_sizes, cometinho, marker=\"o\", color=\"orange\")\n",
        "ax.set_xlabel(\"Beam size\")\n",
        "ax.set_ylabel(\"Cometinho score\")\n",
        "ax.set_title(\"Beam size vs. Cometinho\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hbEUJYySnD5m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Section 3: Minimum Bayes Risk (MBR) decoding\n",
        "\n",
        "As you may have found, MAP as a decision rule does not necessarily result in maximum translation quality. Often, beam search is restricted to use a small beam size of 4 or 5 in order to avoid the aforementioned beam search curse. Therefore, some people have started to look into an alternative decision rule in minimum Bayes risk (MBR). MBR prescribes that the optimal decision from a trained model $P_\\theta(Y|x)$ is:\n",
        "\n",
        "$$\n",
        "y_{\\text{MBR}}\n",
        "= \\arg\\max_{y \\in \\mathcal{Y}}\n",
        " \\mathbb{E}_{P_\\theta(Y \\mid x)}[u(y, Y)].\n",
        "$$\n",
        "\n",
        "Here, $u(\\cdot)$ is a *utility function*: some function that measures the benefit $u(c, r)$ in selecting a candiate $c$ given some reference translation $r$. As you may recognize, the automatic evaluation metrics used in machine translation evaluation (e.g. BLEURT, COMET, etc.) could be used as such a utility function, as long as they function reliably on a sequence level. Of course, at decoding time we do not have access to any actual references. Therefore, the distribution predicted by the machine translation model is used to \"fill in\" this reference probabilistically in the form of an expectation. The MBR solution is then the one translation / candidate in the support of our machine translation model that maximizes *expected utility*.\n",
        "\n",
        "\n",
        "### Approximating MBR\n",
        "\n",
        "Like MAP, computing the MBR solution exactly is typically intractable. Machine translations have an unbounded support over translations and even when restricting length, the size of the support grows exponentially with sequence length. In fact, there are two approximations to be made: *i)* the $\\arg\\max$ is performed over the entire support of the model and therefore needs to be restricted to a smaller set of candidates, and $ii)$ the expectation again would require an enumaration over the entire support, but we can obtain an unbiased estimate of it using a Monte Carlo approximation.\n",
        "\n",
        "In this exercise, we will implement MBR decoding. We suggest using the sampling-based approximation employed in [Eikema & Aziz (2020)](https://aclanthology.org/2020.coling-main.398.pdf):\n",
        "\n",
        "1. **Sample translations**  \n",
        "   Draw $k$ samples $S = \\{y^{(k)}\\}_{k=1}^K \\sim P_\\theta(Y \\mid x)$ via ancestral sampling.\n",
        "\n",
        "2. **Candidate set**  \n",
        "   Use $C = S$ as the set of candidates as well to approximate the $\\arg\\max$.\n",
        "\n",
        "3. **Monte Carlo estimate of expected utility**\n",
        "For each candidate $c$, perform an MC estimate of its expected utility using $S$ as:\n",
        "$$\n",
        "\\mathbb{E}_{P_\\theta(Y \\mid x)}[u(c, Y)] \\approx \\frac{1}{k}\\sum_{k=1}^K u(c, y^{(k)}).\n",
        "$$\n",
        "You can save a utility computation by omitting the utility comparison of a candidate with itself.\n",
        "\n",
        "Now implement MBR decoding by implementing the following three functions:\n",
        "\n",
        "- `estimate_candidate_set`: approximate the $\\arg\\max$.\n",
        "- `estimate_expected_utility`: approximate expected utility using an MC estimate.\n",
        "- `mbr_generate`: put everything together.\n",
        "\n",
        "As a utility function, you can use the Cometinho implementation we provide below."
      ],
      "metadata": {
        "id": "bYTbx0cyJUPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MBRGenerator:\n",
        "\n",
        "    def __init__(self, generator, tokenizer):\n",
        "        self.generator = generator\n",
        "        self.tokenizer = tokenizer\n",
        "        self.comet_scorer = comet.load_from_checkpoint(comet.download_model(\"Unbabel/eamt22-cometinho-da\"))\n",
        "\n",
        "    def _utility_fn(self, src, refs, preds):\n",
        "        # Return one score per (src, ref, pred) triple (vector output).\n",
        "        data = [{\"src\": s, \"mt\": p, \"ref\": r} for s, r, p in zip(src, refs, preds)]\n",
        "        out = self.comet_scorer.predict(data, batch_size=32, gpus=1 if torch.cuda.is_available() else 0)\n",
        "        scores = torch.tensor(out[\"scores\"], dtype=torch.float32)\n",
        "        return scores * 100.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, src_sentences, x, k=5, return_expected_utility=False):\n",
        "        assert k >= 2\n",
        "        candidate_ids = self.estimate_candidate_set(x, k)\n",
        "        expected_utilities = self.estimate_expected_utility(src_sentences, candidate_ids)\n",
        "\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "\n",
        "        Note: Cometinho will need the source sentences untouched by the chat template. Hence,\n",
        "        we pass them separately as src_sentences here.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def estimate_candidate_set(self, x, k, max_sequence_length=256):\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "    def estimate_expected_utility(self, src_sentences, candidate_ids):\n",
        "        \"\"\"\n",
        "        Implement your solution here.\n",
        "        \"\"\"\n",
        "        pass\n",
        "\n",
        "generator = RolloutGeneratorBatched(model, tokenizer, strategy=AncestralSamplingStrategyBatched())\n",
        "mbr_generator = MBRGenerator(generator, tokenizer)\n",
        "src_sentences = [\"Tervetuloa MT Marathon 2025 -tapahtumaan!\",\n",
        "                 \"Tervetuloa MT Marathon 2025:een, viikon mittaiseen konekäännösteknologian huipputapahtumaan, joka järjestetään 25.–29. elokuuta Helsingin yliopiston kampuksella ydinkeskustassa!\"]\n",
        "x = chat_template(src_sentences)\n",
        "response_ids = mbr_generator.generate(src_sentences, x, k=5)\n",
        "responses = tokenizer.batch_decode(response_ids, skip_special_tokens=True)\n",
        "print(\"\\n\")\n",
        "for i, response in enumerate(responses):\n",
        "    print(f\"Input {i+1}: {response}\")"
      ],
      "metadata": {
        "id": "l0xsc6JwI2SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now let's also see how our hyperparameter $k$, the sample size, improves our estimates of the MBR decision rule, and how this affects translation quality. Implement the code snippet below to fill the `average_expected_utility` and `cometinho` arrays for the various sample sizes. You may find that this will take a bit of time. Feel free to shrink down the dataset size further or use smaller sample sizes."
      ],
      "metadata": {
        "id": "1ns25josoCQq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_expected_utility = []  # mean over dataset of expected utility\n",
        "cometinho = [] # system cometinho scores\n",
        "sample_sizes = [2, 4, 6]\n",
        "\n",
        "\"\"\"\n",
        "Implement your solution here.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "q3Ps-dB8I2Id"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And, finally, we also plot our approximation quality of sample size in this sampling-based MBR algorithm to the MBR objective, as well as Cometinho scores. Do you find that increasing sample size improves our estimates of the MBR objective? What about translation quality?\n",
        "\n",
        "Optionally, you can experiment with different strategies for `estimate_candidate_set` that use something else than ancestral samples to approximate the $\\arg\\max$."
      ],
      "metadata": {
        "id": "y3EQAhXw6L6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "ax = axes[0]\n",
        "ax.plot(sample_sizes, average_expected_utility, marker=\"o\")\n",
        "ax.set_xlabel(\"Sample size\")\n",
        "ax.set_ylabel(\"Average expected utility\")\n",
        "ax.set_title(\"Sample size vs. expected utility\")\n",
        "\n",
        "ax = axes[1]\n",
        "ax.plot(sample_sizes, cometinho, marker=\"o\", color=\"orange\")\n",
        "ax.set_xlabel(\"Sample size\")\n",
        "ax.set_ylabel(\"Cometinho score\")\n",
        "ax.set_title(\"Sample size vs. Cometinho\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "r8k_asPZuG1V"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}